input {
  gelf {
    port => 12201
    host => "0.0.0.0"  # 确保 Logstash 监听所有接口
  }
}

filter {
  json {
    source => "message"
    skip_on_invalid_json => true
    target => "json_data"
    remove_field => [ "message" ]
  }

  # 确保解析后的 JSON 数据存在
  if [json_data] {
    # 使用 aggregate 聚合日志，设置任务ID为事件的唯一标识符（如 host）
    aggregate {
      task_id => "%{host}"  # 根据 host 聚合所有来自同一主机的日志
      code => "
        map['logs'] ||= []
        map['logs'] << event.get('json_data')
      "
      push_map_as_event_on_timeout => true
      timeout => 10  # 60秒超时，即每1分钟发送一次
      timeout_tags => ['aggregated']  # 给聚合后的事件添加标签
      timeout_code => "
        if event.get('logs').size > 0
            event.set('logs', event.get('logs'))  # 正确设置 'logs' 字段
        else
            event.cancel
        end
      "
    }
  }

  # 仅保留聚合后的事件，丢弃其他事件
  if "aggregated" not in [tags] {
    drop { }
  }
}

output {
  # 发送 HTTP 请求
  http {
    automatic_retries => 2
    connect_timeout => 60
    request_timeout => 180
    socket_timeout => 60
    format => json
    content_type => "application/json"
    http_method => post
    keepalive => true
    pool_max => 5
    pool_max_per_route => 1
    retry_failed => true
    retryable_codes => [429, 500, 502, 503, 504]
    ssl_enabled => false
    validate_after_inactivity => 180
    url => "http://172.17.0.1:8082/test/log_endpoint"
    message => "%{logs}"  # 发送聚合后的 logs 数组
  }

  # 用于调试输出
  stdout { codec => rubydebug }  # 用于调试，查看聚合后的数据
}
